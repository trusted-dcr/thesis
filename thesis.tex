\documentclass{article}

\usepackage{textcomp}
\usepackage{tikz}
\usepackage{amsmath}

\include{dcr-preamble}

\title{DCR/TEE \\
\large Working subtitle in a trusted execution environment}
\author{Mikkel Gaub \and Malthe Ettrup Kirkbro \and Mads Frederik Madsen}

% Allow line break at ',' in math mode:
\makeatletter
\def\old@comma{,}
\catcode`\,=13
\def,{%
  \ifmmode%
    \old@comma\discretionary{}{}{}%
  \else%
    \old@comma%
  \fi%
}
\makeatother


\begin{document}
\sloppy

\begin{titlepage}
	\maketitle
	\thispagestyle{empty}

	\vspace{\fill}
	\begin{abstract}
	Lorem ipsum...
	\end{abstract}
\end{titlepage}

\clearpage
\setcounter{page}{1}

\tableofcontents

\newpage

\section{Introduction}

\section{Related works}

	An existing platform which partially provides a solution to the goal of this project is Ethereum \cite{ethereum-white-paper}.
	Ethereum is a distributed computing platform based on blockchain technologies, which does, however, also feature a currency, making computations disproportionately expensive, both monetarily and computationally due to the large amounts of computations required by proof-of-work based blockchains.
	Even though SGX is a new technology, it has already been proposed to solve several issues in the blockchain concept. 
	SGX has been used as an intermediate for faster consensus about transactions in \cite{improv-btc}, however it still relies on the underlying blockchain to prevent double-spending.

	In \cite{fastbft} SGX has been utilized to implement a new Byzantine fault tolerance (BFT) consensus algorithm, called \textit{FastBFT}, which solves some of the scalability issues of blockchains.
	This is done by using the \textit{strawman design} where a request is sent to a node, the \textit{primary}, who prepares a vote by distributing parts of a secret to all nodes.
	The secret can be reconstructed given enough parts and then compared to the hash of the secret which is common knowledge.
	This means that consensus can be achieved in only $O(n)$ messages, rather than the $O(n^2)$ messages achieved by other algorithms.
	The reason that Intel SGX is needed for this algorithm is that the primary, who distributes the secrets, can fake being any node as he has all parts of the secret.
	Trusted Execution Environments (TEE) as introduced by Intel SGX, means that these secrets can be computed and distributed without the primary every having access to them.
	A further issue with the strawman design, is that the primary can change the orders of requests, thereby equivocate which request is being voted on.
	This is once again solved by Intel SGX, by numbering requests with a trusted counter that can only be incremented.

	In particular, the Hyperledger Sawtooth \cite{poet} project is interesting as it is closely related to the goals of our project.
	Hyperledger Sawtooth is an ongoing blockchain project, which replaces the need for mining by using a consensus algorithm called Proof of Elapsed Time (PoET).
	In PoET a number of nodes who choose to participate as validators each create a timer using Intel SGX.
	This timer is different per validator and contains a timestamp some time in the future, with a degree of randomness.
	A validator can also check that a timer is valid, using Intel SGX, and also whether or not that timer has expired.
	The first validator to distribute a valid and expired timer to the rest of the validators, is elected leader for the current round of decision-making.	
	This is a strong indicator that Intel SGX can be used for efficiently attaining consensus.

\section{DCR}

\section{Intel SGX}

	Intel SGX is a TEE technology which allows users to define protected areas of memory, so called \textit{enclaves}.
	Intel guarantees \cite{sgx} that any code run and data loaded in the TEE is protected from access by any process running outside of the enclave.
	More specifically, the guarantees encompass \textit{confidentiality}, i.e. no other process can read the data in the enclave, and \textit{integrity}, i.e. no other process can modify the data in the enclave. 
	If the enclave contains secrets which the user wishes to keep, but still does not want to trust external processes with, the enclave can be sealed on disk, essentially encrypting it for later use.
	This means that data can be stored and processed securely, with its security guaranteed by Intel, if done properly.
	An additional feature of Intel SGX is that the result of any code run using Intel SGX can be verified by other users, given the code and the output.

	This is all made possible by unique keys generated during manufacturing and permanently stored inside the fuse array of the processor.
	Some of these keys are known by Intel for the system to be recognized when contacting Intel servers.

	In short, the major innovation in Intel SGX is the option of running hardware secured software, which enables tamper proof messages, where the sender can be verified as being a correct process.
	This eases some of the inherent problems in consensus, as seen in \cite{fastbft} and \cite{poet}.

		\subsection{Enclave}
		What allows SGX enabled CPUs to provide these strong guarantees builds on the following two hardware details:
		\begin{description}
		\item [Processor Reserved Memory (PRM)] is a sequential block of memory reserved for SGX, inaccessible from untrusted software and even hardware.
		\item [Enclave mode] is a mode under which a logical processor gains access to the PRM.
		\end{description}
		Using these hardware facilities SGX defines the concept of an enclave.
		An enclave is, as its name suggests, a software module isolated completely from the rest of the system.
		Its memory is located solely in PRM, preventing access by other processes and enclaves running on the system. 
		The PRM is protected from non-enclave processes by enclave mode. 
		When a process is not in enclave mode and tries to access the PRM, the memory access is denied \cite{intel-sgx-explained}.      
		The PRM of an enclave process is protected from accesses by other processes in enclave mode by the SGX Enclave Control Structure (SECS). 
		The SECS holds meta data about the enclave processes, and among this is a virtual-to-physical memory mapping called the Enclave Page Cache (EPC). 
		Through the EPC an enclave's access to physical PRM is restricted to what has been allocated for the enclave-process \cite{intel-sgx-explained}.

		While SGX provides powerful guarantees trough its enclave concept, it does not guarantee software correctness and will not protect against flawed software.
		Instead SGX encourages developers to isolate a minimal piece of their software, the Trusted Computing Base (TCB), in a trusted enclave environment, and keep the remainder as traditional system processes \cite{sgx-dev-guide}.
		By minimizing the size of the TCB, and thus the amount of code one must trust, common security principles indicate that the chance of security flaws decreases \cite{sgx-dev-guide}.

		In order for an isolated enclave to be useful, communication between trusted and untrusted software is enabled through \textit{enclave calls} (ECALL) and \textit{out calls} (OCALL).
		This interface must be defined at compile-time, specifying an API of ECALLs for the enclave as well as any untrusted services needed as OCALLs, in a \textit{EDL}-file \cite{sgx-dev-guide}.

		When built, an enclave module is a plain binary on the untrusted file system.
		As an enclave under such circumstances would be vulnerable to tampering before initialization, SGX enforces a strict signature policy.
		An enclave must include an \textit{Enclave Signature} containing \textbf{(a)} a hash of the code and initialization data of the enclave, \textbf{(b)} the author's public key and \textbf{(c)} an enclave version/product number.
		During enclave initialization a hardware check is performed, ensuring the Enclave Signature matches the enclave binary loaded from the file system.

		Another powerful tool of an SGX enclave is the ability to read from and write data to an untrusted storage medium while ensuring confidentiality of its contents.
		Such capabilities are needed as the PRM is volatile and will not persist after shutdown.
		In SGX this process is known as \textit{sealing}.
		Sealing allows encryption and decryption using a \textit{Seal Key}, which is confidential to the Enclave Signature.

		\subsection{Attestation}
		Attestation, within the Intel SGX platform, is the action of verifying the existence of specific enclaves.
		The applications of this are two-fold, in the case where there are multiple enclaves running locally on the same CPU and in the case where enclaves need to communicate to enclaves running on external CPUs.
		These two situations are handled by two different processes, aptly named local and remote attestation.
  
		Local attestation builds on a secret fused into the CPU.
		An enclave can use an SGX instruction to generate a report uniquely identifying the enclave.
		This report is MACed with a key derived from the fused secret and enclave identifier.
		It is not possible to fake such a MACed report, as the MACing happens in hardware components that ensure that the report corresponds to the caller enclave.
		The fused secret is not directly accessible by software components, but can only be used by certain hardware instructions that protect against malicious use.
		An attestation challenger will ask for a MACed report from the client enclave, and after receiving it derive the same key to verify the MAC.
		Because a report can only be created by the enclave it describes, the challenger can be certain of which enclave the client is running if the MAC is valid.
		To prevent replays the MACed report is allowed to contain a block of arbitrary data, in which the challenger can require a nonce.

		Remote attestations build on the same concept of a report, but as challenger and client are now on different CPUs, they no longer hold the same fused secret.
		Instead remote attestation relies on the group signature scheme Enhanced Privacy ID (EPID) and a third party issuer.
		Each SGX enabled CPU is granted an EPID Member Private Key by the third party issuer some time after manufacturing.
		Like the fused secret, this private key is not directly accessible to an enclave.
		Under the EPID scheme, all issuer generated private keys share the same public key (EPID Group Public Key).
		When remote attestation takes place, the client enclave generates a report and attests locally with the Quoting Enclave (QE).
		The QE, now convinced of the client enclave's identify, strips the MAC off the report and instead signs it with the EPID Member Private Key, which the QE has special privileges to access\footnote{Enclaves signed by Intel have special privileges throughout SGX. This allows SGX to implement complicated SGX instructions in software.}.
		The remote challenger can verify the signed report with the EPID Group Public Key and be sure that \textbf{(a)} the report is signed by an EPID Member Private Key, \textbf{(b)} EPID Member Private Keys are granted secret to QEs, and \textbf{(c)} QEs will not sign false reports.

		\subsection{Monotonic counters}
		One of the functions provided by Intel SGX, which is especially relevant for this project, is the access to \textit{trusted monotonic counters}.
		As indicated by the name, monotonic counters are integer counters, that can only be incremented.
		They are implemented as a block of non-volatile memory accessible only through SGX instructions, protecting against replay attacks.



\section{Problem}
	The system \cite{schneider_implementing_1990} should support running a distributed DCR graph between multiple parties, under the assumption that any number of those parties will attempt to forge, collude, perjure or otherwise act maliciously, i. e. exhibiting byzantine failures.
	These distributed DCR graphs should always be accessible by all parties, connectivity permitting, and transparency must be ensured for all parties, in that they should always be aware of whether or not activities controlled by that party can be executed at any given time.

	% 1. There must be a happened-before relationship between the execution of an activity and the executions of its dependent activities % Studer Lamports FSM
	% 2. All serializations of the happened-before relationships must be valid execution sequences in DCR % Og opnå enighed om dem

	\section{Definitions}
	\noindent
	Workflow: $G=(V,E)$ \\
	Activities: $V=\{v_1,\ \dots, v_n\}$ \\
	Activity States: $S=\{s_1,\ \dots, s_n\}$, where $s_i \subseteq \{Executed, Pending, Included\}$ corresponding to $V_i$.\\
	Relation: $R=($condition, response, milestone, include, exclude$)$, $e=(v_x, v_y, r_i)$, $E=\{e_1,\ \dots, e_m\}$\\
	Processes: $P =\{p_1,\ \dots, p_o\}$.\\
	History, $H_p(t)$: Sequence of activity executions up to, and at time $t$, as perceived by given process.\\
	Execution: $(v_i, t, p)$, where $v_i$ is the activity executed at time $t$, executed by process $p$.\\
	Dependant activities: An activity, $v_1$ is said to be dependant on another activity, $v_2$, when the DCR rules allowing $v_1$ to be executed, depends on the state of $v_2$. \\
	% Processor: $C$ is said to own a number of specific processes. Any process can only be owned by a single processor. \\
	% Activity responsibility: ... \\

	\section{Requirements}
	\begin{description}
		\item[Consensus]:
			\begin{description}
				\item[Termination] Eventually each correct process sets its decision value (single activity state)
				\item[Partial agreement] % Decision vector = activity state
								Bottom is allowed as substitute for any activity state, unless that activity is owned by process or dependant on one such process. % Hvad er agreement
				\item[Integrity] If $p_i$ is correct, then all correct processes decide on $v_i$, or $\bot$ as the ith component of their vector. % Omformuler. Kan ikke genbruge i på den her måde
			\end{description}
		% \item[Concurrency]: DCR-rules for concurrency (only independent activities can be concurrent) (Tie-breaking). Nødvendig? Argumenter hvorfor Agreement + integrity giver concurrency løsning
		\item[Correctness]: Any state transition must be permitted by DCR logic or they will be rejected by correct nodes.
		\item[Non-repudiation]: $v_i$ must be provably proposed by $p_i$, within the bounds specified by any applied cryptography.
		% \item[Co-non-repudiation?]:
	\end{description}

	\section{Scenarios} % Omformuleres til at bruge definitions.
	\begin{description}
		\item[Scenario 1]
			$A\crel B$\\
			Alice must never decide $\bot$ for $A$. Bob must never decide $\bot$ for either $A$ or $B$.
		\item[Scenario 2]
			$A\crel B$, $A\crel C$\\
			Alice, Bob and Charlie must always decide the same value for $A$.\\
			Problem: Prevent Alice from telling Bob the state, but not Charlie. (Reduces to FLP.)
		\item[Scenario 3]
			$A\erel B$, $B\crel C$\\
			Alice and Bob must agree on the state of $A$, Bob and Charlie must agree on the state of $B$, only Charlie must agree on the value of $C$.
		\item[Scenario 4]
			$A\erel B$, $B\erel A$\\
			Both Alice and Bob must agree on both $A$ and $B$.\\
			Problem: Concurrency.
		\item[Scenario 5]
			$A\irel C$, $A\irel D$, $B\erel C$, $B\erel D$\\
			Both Alice and Bob must agree on only $A$ and $B$, respectively. Charlie must agree on everything except $D$, and Dahlia must agree on everything except $C$\\
			Problem: Concurrency (expanded), not serially equivalent if $C$ is included and $D$ is excluded after $A$ and $B$ have executed concurrently.
	\end{description}

	\section{Network topology}
	Due to the algorithm not requiring the executioner of an event to have the global state of the workflow at the time of execution, any state changes are only propogated to the relevant parties.
	This means that any single crash could potentially make collecting the global state, or history of executions, impossible, as the localized state of the crashed peer would be lost.
	To make the system more tolerant to crashes, the state of each event is tracked by a number of peers.
	This means that all locks need to be made on a subsystem rather than a single peer.
	As this lock needs to prevent others from locking simultaneously, $\dfrac{m}{2} +1$ locks are required for each peer, where $m$ is the number of peers tracking the state of that event.

	\subsection{Peer distribution}
	Given that the relations of a workflow are fixed at creation, the peers can be distributed efficiently by assigning events frequently locked simultaneously to the same peers.
	This can be accomplished by the following steps:
	\begin{itemize}
		\item Each event is assigned a set of peers, called the primaries of that event, containing at least one peer.
		\item The primaries of each event are added to each event on which the execution of the event tracked by the primaries could incur a lock. The peers added this way are called secondaries for that event.
	\end{itemize}

	The peer-to-peer network is then configured in such a way that each peer is a neighbour to any peer which tracks the same events.
	This means that the execution of an event can be performed by a primary, by that primary attempting to lock all of its neighbours and checking whether or not the acquired locks are sufficient in order to perform the execution, meaning that the set of locked peers forms a quorum for each event requiring a lock.

	\section{Global state collection}
	% global state collection, snapshot?
	% Crash recovery

	\section{Algorithm}
	\subsection{Background}
	For any algorithm to guarantee workflow correctness of a distributed DCR graph in an asynchronous setting, the algorithm must (1) push executions to effected neighbours and (2) synchronize the executions of non-independent events.  
	This section provides the reasoning behind these requirements.

	We will describe the algorithm under the most general case where each event is handled by a separate peer.
	For simplicity e.g. $A$ will refer to both the event and the peer responsible for it.
	% We will show that this workflow is enough to describe a general algorithm when $A$ is executed.
	% Something with events only chaning state one degree forward and thereby enabledness two degrees forward. 
	% Why?

	% Effects must push execution state
	Consider the events $A$, $B$ and $C$, where $A$ has relation $AB$ to $B$, and $B$ has relation $BC$ to $C$.
	When $AB$ is an effect ($A\irel B$, $A\erel B$, $A\rrel B$), $B$ must be informed of any execution of $A$.
	To see this is the case, consider the workflow $(\{D, E, F\}, \{D\erel F, E\irel F\})$.
	As $F$ is not immediately informed about executions of $D$ and $E$, an execution attempt at $F$ must query $D$ and $E$ to determine the inclusion state of $F$.
	Such a query would require an ordering of executions of $D$ and $E$, which is not generally possible in an asynchronous setting.

	% AB effects must synchronize
	When an execution happens on an event with an effect to another event, some degree of synchronization must occur between a subset of peers.
	To see this is the case, consider the workflow $(\{D, D', E, E'\}, \{D\irel E, D\erel E', D'\erel E, D'\irel E'\})$.
	We showed executions must be propagated along effects in an asynchronous setting.
	Thus when executing $D$ and $D'$, messages $M_{D\irel E}$, $M_{D\erel E'}$, $M_{D'\erel E}$ and $M_{D'\irel E'}$ are required.
	The ordering of these messages can not generally be guaranteed in an asynchronous setting.
	To see why synchronization must occur, consider the message arrival sequence $(M_{D\irel E}, M_{D'\erel E}, M_{D'\irel E'}, M_{D\erel E'})$, resulting in an invalid workflow state where both $E$ and $E'$ are excluded.

	To achieve the required synchronization the algorithm needs only to provide synchronization in the cases described in section \ref{subsec:locking}.

	% Execute A, lock B
	% Execute B, lock A

	% AB effects, BC constaints must synchronize
	% When $AB$ is an effect and $BC$ is a constraint ($B\crel C, B\mrel C$),

	% Forward locking
	% Backward locking
	% Foward-backward locking

	% When $A$ is executed:\\
	% $B$ must be locked if:\\
	% $AB$ is an effect (include, exclude, response)\\
	% $C$ must be locked if:\\
	% $AB$ is a constraining effect, and $BC$ is a constraint (condition, milestone), in the following configuration:\\
	% \begin{itemize}
	% 	\item $A \irel B \crel C$ (B is excluded)
	% 	\item $A \irel B \mrel C$ (B is excluded and pending)
	% 	\item $A \rrel B \mrel C$ (B is not pending)
	% \end{itemize}

	\subsection{Locking}
	\label{subsec:locking}

	Since the semantics of DCR only allows the execution of an activity to affect events in the neighbourhood and the second neighbourhood of the executing event, locking must be performed within this set.
	In a graph, $G(E,V)$, we define the set of events that could be incident to locking, $L$, on the execution of an event, $v$, as $L(v)$.

	Assuming that locking the entirety of $L(v)\ =\ N(N(v))\ \cup\ N(v)\ \cup\ v$ when executing $v$ is suboptimal, there are three intuitive methods of locking, here compared on three non-trivial graphs:

	\begin{description}
		\item[1: Exclude-include] $A \erel D, A \irel C, B \erel C, B \irel D$
		\item[2: Include condition] $A \irel C, C \crel B, B \irel D, D \crel A$ where $C$ and $D$ are excluded.
		\item[3: Non-modifying to condition] $A \irel C, C \crel B, B \irel D, D \crel A, E \erel C$ where $D$ is excluded.
	\end{description}

	\begin{description}
		\item[Backwards locking] $L(v)\ =\ N(N(v))\ \cup\ N(v)\ \cup\ v$ where $N(v)$ is the in-neighbourhood of $v$. This locking does not work for graph 1, as A and B do not need to acquire locks on C and D. This means that the end-state of the workflow is subject to race conditions.
		\item[All adjacent] $L(v)\ =\ N(v)\ \cup\ v$ where $N(v)$ is the neighbourhood of $v$. This solves the problem of \emph{backwards locking} for graph 1, as $C$ and $D$ would be incident to locking by both $A$ and $B$.
		\item[Forwards locking] $L(v)\ =\ N(N(v))\ \cup\ N(v)\ \cup\ v$ where $N(v)$ is the out-neighbourhood of $v$. While there is no inherent problem with the \emph{all adjacent} locking rules, an exectution of event $A$ in graph 3 using \emph{all adjacent} would require two locks: on $C$ and $D$, while using \emph{forwards locking} would only incur locking on $C$ as the state-change incurred by the include relation on $C$ does not allow $B$ to be executed.
	\end{description}

	It seems that, given the state of all relevant nodes, the locking mechanism described as \emph{forward locking} minimizes the amount of locks needed for any execution.
	This optimization is, however, only possible when the state of $C$ is known and since locking is only performed forwards, there is no implicit way to propogate executions backwards.
	This means that $A$ does not know the state of $C$ on execution and would have to do one of two options when executing:
	\begin{itemize}
		\item $A$ sends a locking message to $C$, telling it to lock $B$ if necessary.
		\item $A$ sends a locking message to $C$ and one to $B$, but gives $B$ the option to forego locking if the state of $C$ would be unchanged by the execution of $A$.
	\end{itemize}
	Both options result in the minimum amount of locks, but the first option has fewer messages, as it only results in two messages.
	However, the first option has the disadvantage of potentially taking twice as long, assuming that transporting a message between $A$ and $C$ takes the same amount of time as between $A$ and $B$.
	These various advantages of different methods means that finding one optimal solution is not possible, as it is dependant on the specific workflow and the topology of the network at a given time. 

	% SGX role regarding fault tolerance

	\section{Consensus}
	N processes: $p_1, p_2 \dots, p_n$\\
	N proposed values: $\{v_1, v_2 \dots, v_n\}$, where each $v$ is a value from unspecified set $D$.
	N decision variables: $\{d_1, d_2, \dots, d_n\}$
	Process $p_i$ moves from \textit{undecided} state to \textit{decided} by proposing $v_i$ and setting $d_i$ s.t:\\
	\begin{description}
		\item[Termination]: Eventually each correct process sets its decision variable.
		\item[Agreement]: The decision value of all correct processes is the same.
		\item[Integrity]: If the correct processes all proposed the same value, then any correct process in the decided state has chosen that value.
	\end{description}
	
	\noindent
	\subsection{Distributed DCR (D-DCR)}
	Given the distributed DCR graph $G$.
	\begin{description}
		\item[Consensus]:
			\begin{description}
				\item[Termination] Eventually each correct process sets its decision value (single activity state)
				\item[Agreement] Decision vector = activity states
								1. Eventual agreement: We can poll for an history, after which agreement on current state is reached.
								2. Partial agreement: Bottom is allowed as substitute for any activity state (not if you're responsible for that activity)
				\item[Integrity] If $p_i$ is correct, then all correct processes decide on $v_i$, or $\bot$ as the ith component of their vector.
			\end{description}
		\item[Concurrency]: DCR-rules for concurrency (only independent activities can be concurrent) (Tie-breaking)
		\item[Correctness]: For a correct peer to propose a changed activity state, the new activity state must be the result of a valid activity execution in the last decided decision vector. We assume an agreed upon genesis state.
		\item[Non-repudiation]: $v_i$ must be provably proposed by $p_i$, within a negligible probability.
		% \item[Co-non-repudiation?]:
	\end{description}

	\noindent
	\subsection{Reduction from D-DCR to consensus (no concurrency)}
	(Assume initial state is agreed upon)\\
	Let $D$ be the set of possible decision vectors.\\
	Any correct process will only choose a decision vector that is obtainable by executing an activity that is in an executable state in the last agreed upon decision vector. (Correctness)\\
	% The decision vector must be proposed by an actor with execution rights of the 
	Use consensus to decide on a possible execution.\\

	\noindent
	\subsection{Reduction from consensus to D-DCR}
	For each $v$ in $D$ create an activity $a$ in a DCR graph.\\
	Let all activities exclude all other activities (no concurrency).\\
	All actors can execute all activities.
	When a peer chooses a $v_i$, model this as trying to execute the corresponding $a_i$.
	Run D-DCR.\\
	When an execution is completed, set $d_i$ as the $v_i$ corresponding to the executed activity.

	\section{Implementation}

	\section{Discussion}

		\subsection{SGX security}
	    All aforementioned hardware guarantees are given by Intel, but since very little of the proprietary technology is documented, all security properties of SGX rely on the correctness of Intel's hardware implementation.
	    Historically Intel has had vulnerabilities in similar hardware components, like the CVE-2017-5689 security incident exposed in \cite{silent-bob}, allowed unprivileged access to the Intel Active Management Technology.

	    SGX claims to provide confidentiality and integrity for running enclaves.
	    This is ensured by preventing untrusted software access to the PRM and enclaves access to other regions of the PRM besides their exclusive region.
	    However, while it is not possible to breach the integrity of PRM, \cite{intel-sgx-explained} shows several issues regarding confidentiality.
	    Because enclaves use the system page table even for data residing on PRM, it is possible for untrusted software to learn the page access order by manipulating the OS controlled page table.
	    Another possible attack vector is to perform cache timing by cleverly choosing the physical memory position of enclave memory pages on set associative caches.
	    By mapping snooping software to the same cache set as the enclave, the snooping software can evict the enclave's memory from the cache and use timing to figure out if it is accessed again.
	    Lastly, processors with hyper-threading support are vulnerable to instruction snooping, as a snooping process sharing physical core with an enclave can use performance counters to determine which instructions the out-of-order scheduler is able to run in parallel with the enclave.
	    The presence of these attacks means that enclaves using data-dependent memory access will likely not ensure confidentiality.
	    However, to our knowledge, no publicly known vulnerabilities exist on the integrity guarantee. 
	    And since our system has very few confidentiality requirements, the known vulnerabilities do not seem problematic for the purposes of this project.

	    As mentioned SGX does not protect against flawed software, so it is up to the developer to prevent side-channel attacks through OCALLs that might expose secret data.
	    A noteworthy mention to illustrate the vigor needed is the common pitfall mentioned in \cite{sgx-dev-guide} when using ECALLs and OCALLs.
	    Because enclaves are compiled using a standard \cpp-compiler, structure padding is likely to happen.
	    The SGX environment does not protect against leaking secret information through uninitialized structure padding when passing structures in ECALLs and OCALLs -- this, and other common security pitfalls, are not part of any SGX security guarantees.
	    Intel recommends to always clear secrets from the enclave memory after use in \cite{sgx-dev-guide}, regardless of the guarantees given by the PRM, underlining the risk of unintended vulnerabilities.

	\section{Conclusion}

	\newpage
	\bibliography{bibliography}{}
	\bibliographystyle{plain}

	\newpage

	\appendix
	% \section{Code}

\end{document}